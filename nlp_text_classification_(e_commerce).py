# -*- coding: utf-8 -*-
"""NLP - Text Classification (E-commerce).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Mz1GlE8k3f1Es9i_aUKL0qP7HPcSKLXd

### **The objective of the project is to classify a product into the four categories Electronics, Household, Books and Clothing & Accessories, based on its description available on the e-commerce platform.**
"""

import time
import json

# Data manipulation
import numpy as np
import pandas as pd

# NLP
import string, re, nltk
from string import punctuation
from nltk.corpus import stopwords
nltk.download('stopwords')
from nltk.stem.porter import PorterStemmer
from nltk.stem import WordNetLemmatizer
nltk.download('wordnet')
from textblob import TextBlob

# TF-IDF
from sklearn.feature_extraction.text import TfidfVectorizer

from sklearn.model_selection import train_test_split

# Classifiers
from sklearn.linear_model import LogisticRegression
from sklearn import svm
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier

# Model evaluation
from sklearn import metrics
from sklearn.metrics import accuracy_score

data = pd.read_csv('/content/ecommerceDataset.csv', names = ['label', 'description'])
data = data[['description', 'label']]

data

# Example description
data['description'].iloc[0]

# Missing values
data.isna().sum()

# Duplicate observations
data.duplicated().sum()

data.dropna(inplace = True) # Dropping observations with missing values
data.drop_duplicates(inplace = True) # Dropping duplicate observations
data.reset_index(drop = True, inplace = True) # Resetting index

data.shape

# Manually encoding of labels
label_dict = {'Electronics': 0, 'Household': 1, 'Books': 2, 'Clothing & Accessories': 3}
data.replace({'label': label_dict}, inplace = True)

data

data['label'].value_counts()

"""Train-Validation-Test Split"""

# Feature-target split
X = data.drop('label', axis = 1)
y = data['label']

# Train-test split (from complete data)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 40)
data_train = pd.concat([X_train, y_train], axis = 1)

# Validation-test split (from test data)
X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size = 0.5, random_state = 40)
data_val, data_test = pd.concat([X_val, y_val], axis = 1), pd.concat([X_test, y_test], axis = 1)

print('shape of data_train = ',data_train.shape)
print('shape of data_test = ',data_test.shape)
print('shape of data_val = ',data_val.shape)

"""## Text Normalization

* Convertion to Lowercase Removal of Whitespaces
* Removal of Punctuations
* Removal of Unicode Characters
* Substitution of Acronyms
* Substitution of Contractions
* Removal of Stop Words
* Spelling Correction
* Stemming and Lemmatization
* Discardment of Non-alphabetic Words
* Integration of the Processes
* Implementation on Product Description
"""

# Converting to lowercase
def convert_to_lowercase(text):
  return text.lower()

# Removing whitespaces
def remove_whitespace(text):
  return text.strip()

# Removing punctuations
def remove_punctuation(text):
  punct_str = string.punctuation
  punct_str = punct_str.replace("'", "") # discarding apostrophe from the string to keep the contractions intact
  return text.translate(str.maketrans("", "", punct_str))

# Removing HTML tags
def remove_html(text):
  html = re.compile(r'<.*?>')
  return html.sub(r'', text)

## Example
# text = '<a href = "https://www.kaggle.com/datasets/ecommerce-text-classification"> Ecommerce Text Classification </a>'
# print("Input: {}".format(text))
# print("Output: {}".format(remove_html(text)))

# Removing emojis
def remove_emoji(text):
  emoji_pattern = re.compile("["
                          u"\U0001F600-\U0001F64F"  # emoticons
                          u"\U0001F300-\U0001F5FF"  # symbols & pictographs
                          u"\U0001F680-\U0001F6FF"  # transport & map symbols
                          u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                          u"\U00002702-\U000027B0"
                          u"\U000024C2-\U0001F251"
                          "]+", flags = re.UNICODE)
  return emoji_pattern.sub(r'', text)

# # Example
# text = "This innovative hd printing technique results in durable and spectacular looking prints ðŸ˜Š"
# print("Input: {}".format(text))
# print("Output: {}".format(remove_emoji(text)))

# Removing other unicode characters
def remove_http(text):
  http = "https?://\S+|www\.\S+" # matching strings beginning with http (but not just "http")
  pattern = r"({})".format(http) # creating pattern
  return re.sub(pattern, "", text)

# # Example
# text = "It's a function that removes links starting with http: or https such as https://en.wikipedia.org/wiki/Unicode_symbols"
# print("Input: {}".format(text))
# print("Output: {}".format(remove_http(text)))

# Substitution of Acronyms

# Dictionary of acronyms
acronyms_url = 'https://raw.githubusercontent.com/sugatagh/E-commerce-Text-Classification/main/JSON/english_acronyms.json'
acronyms_dict = pd.read_json(acronyms_url, typ = 'series')

print("Example: Original form of the acronym 'fyi' is '{}'".format(acronyms_dict['fyi']))

# Dataframe of acronyms
print('\n')
pd.DataFrame(acronyms_dict.items(), columns = ['acronym', 'original']).head()

# Function to convert contractions in a text
def convert_acronyms(text):
  text_words = text.split()
  for i, word in enumerate(text_words):
      if word.lower() in acronyms_dict:
          text_words[i] = acronyms_dict[word]
  return ' '.join(text_words)

text = "btw you've to fill in the details including dob"
print("Input: {}".format(text))
print("Output: {}".format(convert_acronyms(text)))

# Substitution of Contractions

# Dictionary of contractions
contractions_url = 'https://raw.githubusercontent.com/sugatagh/E-commerce-Text-Classification/main/JSON/english_contractions.json'
contractions_dict = pd.read_json(contractions_url, typ = 'series')

# Dataframe of contractions
pd.DataFrame(contractions_dict.items(), columns = ['contraction', 'original']).head()

# Function to convert contractions in a text
def convert_contractions(text):
  text_words = text.split()
  for i, word in enumerate(text_words):
      if word.lower() in contractions_dict:
          text_words[i] = contractions_dict[word]
  return ' '.join(text_words)

text = "he's doin' fine"
print("Input: {}".format(text))
print("Output: {}".format(convert_contractions(text)))

# Stopwords
stops = stopwords.words("english") # stopwords
addstops = ["among", "onto", "shall", "thrice", "thus", "twice", "unto", "us", "would"] # additional stopwords
allstops = stops + addstops

# Function to remove stopwords from a list of texts
def remove_stopwords(text):
  words = text.split()
  return " ".join([word for word in words if word not in allstops])

# Spelling Correction
def spellchecker(text):
  word_list = text.split()
  word_list_corrected = []
  for word in word_list:
      corr_spell = TextBlob(word)
      word_list_corrected.append(str(corr_spell.correct()))

  text_corrected = " ".join(word_list_corrected)
  return text_corrected

# Stemming
stemmer = PorterStemmer()
def text_stemmer(text):
  word_list = text.split()
  text_stem = [stemmer.stem(word) for word in word_list]
  return " ".join(text_stem)

# Lemmatization
lemmatizer = WordNetLemmatizer()
def text_lemmatizer(text):
  word_list = text.split()
  text_lemm = [lemmatizer.lemmatize(word, pos="v") for word in word_list]
  return " ".join(text_lemm)

# Discardment of Non-alphabetic Words
def discard_non_alpha(text):
  word_list = text.split()
  word_list_non_alpha = [word for word in word_list if word.isalpha()]
  text_non_alpha = " ".join(word_list_non_alpha)
  return text_non_alpha

# Integration of the Processes
def text_normalizer(text):
  text = convert_to_lowercase(text)
  text = remove_whitespace(text)
  text = re.sub('\n' , '', text) # converting text to one line
  text = re.sub('\[.*?\]', '', text) # removing square brackets
  text = remove_http(text)
  text = remove_punctuation(text)
  text = remove_html(text)
  text = remove_emoji(text)
  text = convert_acronyms(text)
  text = convert_contractions(text)
  text = remove_stopwords(text)
  #text = spellchecker(text)
  #text = text_stemmer(text)
  text = text_lemmatizer(text)
  text = discard_non_alpha(text)
  return text

# Example
# text = "Combine all functions into 1 SINGLE FUNCTION ðŸ™‚ & apply on @product #descriptions https://en.wikipedia.org/wiki/Text_normalization"
# print("Input: {}".format(text))
# print("Output: {}".format(text_normalizer(text)))

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # Implementing text normalization
# data_train_norm, data_val_norm, data_test_norm = pd.DataFrame(), pd.DataFrame(), pd.DataFrame()
# 
# data_train_norm['normalized description'] = data_train['description'].apply(text_normalizer)
# data_val_norm['normalized description'] = data_val['description'].apply(text_normalizer)
# data_test_norm['normalized description'] = data_test['description'].apply(text_normalizer)
# 
# data_train_norm['label'] = data_train['label']
# data_val_norm['label'] = data_val['label']
# data_test_norm['label'] = data_test['label']
# 
# data_train_norm

"""# Text Vectorization"""

# Features and labels
X_train_norm, y_train = data_train_norm['normalized description'].tolist(), data_train_norm['label'].tolist()
X_val_norm, y_val = data_val_norm['normalized description'].tolist(), data_val_norm['label'].tolist()
X_test_norm, y_test = data_test_norm['normalized description'].tolist(), data_test_norm['label'].tolist()

# TF-IDF vectorization
TfidfVec = TfidfVectorizer()
X_train_tfidf = TfidfVec.fit_transform(X_train_norm)
X_val_tfidf = TfidfVec.transform(X_val_norm)
X_test_tfidf = TfidfVec.transform(X_test_norm)

#TF-IDF Baseline Modeling

# Classifiers
names = [
    "Logistic Regression",
    "Linear SVM",
    "Random Forest",
    "XGBoost"
]

models = [
    LogisticRegression(max_iter = 1000),
    svm.SVC(kernel = 'linear'),
    RandomForestClassifier(n_estimators = 100),
    XGBClassifier()
]

# Function to return summary of baseline models
def score(X_train, y_train, X_val, y_val, names = names, models = models):
  score_df, score_train, score_val = pd.DataFrame(), [], []

  for model in models:
      model.fit(X_train, y_train)
      y_train_pred, y_val_pred = model.predict(X_train), model.predict(X_val)
      score_train.append(accuracy_score(y_train, y_train_pred))
      score_val.append(accuracy_score(y_val, y_val_pred))

  score_df["Classifier"], score_df["Training accuracy"], score_df["Validation accuracy"] = names, score_train, score_val
  score_df.sort_values(by = 'Validation accuracy', ascending = False, inplace = True)
  return score_df

# Summary of baseline models
score(X_train_tfidf, y_train, X_val_tfidf, y_val, names = names, models = models)